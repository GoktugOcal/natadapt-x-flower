{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "import warnings\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import base64\n",
    "from traceback import print_exc\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.utils.data.sampler as sampler\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "from tqdm import tqdm\n",
    "from time import sleep, time\n",
    "\n",
    "import logging\n",
    "\n",
    "from non_iid_generator.customDataset import CustomDataset\n",
    "\n",
    "DEVICE = os.environ[\"TORCH_DEVICE\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_dataset_path, test_dataset_path):\n",
    "    \"\"\"Load CIFAR-10 (training and test set).\"\"\"\n",
    "    batch_size = 128\n",
    "\n",
    "    train_data = pickle.load(open(train_dataset_path, \"rb\"))\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True)\n",
    "\n",
    "    test_data = pickle.load(open(test_dataset_path, \"rb\"))\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "def fine_tune(model, no_epoch, train_loader, print_frequency=100):\n",
    "    '''\n",
    "        short-term fine-tune a simplified model\n",
    "        \n",
    "        Input:\n",
    "            `model`: model to be fine-tuned.\n",
    "            `iterations`: (int) num of short-term fine-tune iterations.\n",
    "            `print_frequency`: (int) how often to print fine-tune info.\n",
    "        \n",
    "        Output:\n",
    "            `model`: fine-tuned model.\n",
    "    '''\n",
    "\n",
    "    # Data loaders for fine tuning and evaluation.\n",
    "    batch_size = 128\n",
    "    momentum = 0.9\n",
    "    weight_decay = 1e-4\n",
    "    finetune_lr = 0.001\n",
    "\n",
    "    # train_loader, val_loader = load_data(train_dataset_path, test_dataset_path)\n",
    "\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    _NUM_CLASSES = 10\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        finetune_lr, \n",
    "        momentum=momentum,\n",
    "        weight_decay=weight_decay)\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    model.train()\n",
    "    # dataloader_iter = iter(train_loader)\n",
    "    print(\"Fine tuning started.\")\n",
    "    for i in range(no_epoch):\n",
    "        if i % print_frequency == 0:\n",
    "            print('Fine-tuning Epoch {}'.format(i))\n",
    "            sys.stdout.flush()\n",
    "        for i, (input, target) in enumerate(tqdm(train_loader)):\n",
    "\n",
    "            # (input, target) = next(dataloader_iter)\n",
    "            \n",
    "            # Ensure the target shape is sth like torch.Size([batch_size])\n",
    "            if len(target.shape) > 1: target = target.reshape(len(target))\n",
    "\n",
    "            target.unsqueeze_(1)\n",
    "            target_onehot = torch.FloatTensor(target.shape[0], _NUM_CLASSES)\n",
    "            target_onehot.zero_()\n",
    "            target_onehot.scatter_(1, target, 1)\n",
    "            target.squeeze_(1)\n",
    "            input, target = input.to(DEVICE), target.to(DEVICE)\n",
    "            target_onehot = target_onehot.to(DEVICE)\n",
    "\n",
    "            pred = model(input)\n",
    "            loss = criterion(pred, target_onehot)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()  # compute gradient and do SGD step\n",
    "            optimizer.step()\n",
    "\n",
    "            del loss, pred\n",
    "\n",
    "    return model\n",
    "\n",
    "######################\n",
    "######################\n",
    "\n",
    "def load_data_cifar(dataset_path = \"./data/\"):\n",
    "    batch_size = 128\n",
    "\n",
    "    train_dataset = datasets.CIFAR10(root=dataset_path, train=True, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4), \n",
    "        transforms.Resize(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ]))\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size, \n",
    "        pin_memory=True,\n",
    "        shuffle=True)#, sampler=train_sampler)\n",
    "\n",
    "\n",
    "    val_dataset = datasets.CIFAR10(root=dataset_path, train=False, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.Resize(224), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ]))\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        pin_memory=True) #, sampler=valid_sampler)\n",
    "    return train_loader, test_loader\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_id = 0\n",
    "\n",
    "train_dataset_path = f\"./data/Cifar10/train/{client_id}.pkl\"\n",
    "test_dataset_path = f\"./data/Cifar10/test/{client_id}.pkl\"\n",
    "trainloader, testloader = load_data(train_dataset_path, test_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifartrainloader, cifartestloader = load_data_cifar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"models/alexnet/model_cpu.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning started.\n",
      "Fine-tuning Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 6/14 [00:05<00:06,  1.14it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/goktug/Desktop/thesis/netadapt-x-flower/dataset_test.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/dataset_test.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m fine_tune(model, \u001b[39m5\u001b[39;49m, trainloader, print_frequency\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32m/home/goktug/Desktop/thesis/netadapt-x-flower/dataset_test.ipynb Cell 6\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/dataset_test.ipynb#W4sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(pred, target_onehot)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/dataset_test.ipynb#W4sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/dataset_test.ipynb#W4sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()  \u001b[39m# compute gradient and do SGD step\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/dataset_test.ipynb#W4sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/dataset_test.ipynb#W4sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39mdel\u001b[39;00m loss, pred\n",
      "File \u001b[0;32m~/python_envs/easyfl/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/python_envs/easyfl/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fine_tune(model, 5, trainloader, print_frequency=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine tuning started.\n",
      "Fine-tuning Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 166/391 [02:24<03:19,  1.13it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/goktug/Desktop/thesis/netadapt-x-flower/dataset_test.ipynb Cell 7\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/dataset_test.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m fine_tune(model, \u001b[39m5\u001b[39;49m, cifartrainloader, print_frequency\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32m/home/goktug/Desktop/thesis/netadapt-x-flower/dataset_test.ipynb Cell 7\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/dataset_test.ipynb#W5sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(pred, target_onehot)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/dataset_test.ipynb#W5sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/dataset_test.ipynb#W5sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()  \u001b[39m# compute gradient and do SGD step\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/dataset_test.ipynb#W5sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/dataset_test.ipynb#W5sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m \u001b[39mdel\u001b[39;00m loss, pred\n",
      "File \u001b[0;32m~/python_envs/easyfl/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/python_envs/easyfl/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fine_tune(model, 5, cifartrainloader, print_frequency=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = f\"./data/Cifar10/server/train.pkl\"\n",
    "test_dataset_path = f\"./data/Cifar10/server/test.pkl\"\n",
    "trainloader, testloader = load_data(train_dataset_path, test_dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from non_iid_generator.customDataset import CustomDataset\n",
    "\n",
    "\n",
    "train_dataset_path = os.path.join(\"./data/32_Cifar10_NIID_56c_a03\", \"train\", \"1.pkl\")\n",
    "train_data = pickle.load(open(train_dataset_path, \"rb\"))\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=128,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easyfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
