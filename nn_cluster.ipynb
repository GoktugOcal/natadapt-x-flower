{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "import warnings\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import base64\n",
    "from traceback import print_exc\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torch.utils.data.sampler as sampler\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from tqdm import tqdm\n",
    "from time import sleep, time\n",
    "\n",
    "import logging\n",
    "\n",
    "from non_iid_generator.customDataset import CustomDataset\n",
    "\n",
    "DEVICE = os.environ[\"TORCH_DEVICE\"]\n",
    "DEVICE = \"cuda\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_dataset_path, test_dataset_path):\n",
    "    \"\"\"Load CIFAR-10 (training and test set).\"\"\"\n",
    "\n",
    "    batch_size = 128\n",
    "    momentum = 0.9\n",
    "    weight_decay = 1e-4\n",
    "    finetune_lr = 0.001\n",
    "\n",
    "    train_data = pickle.load(open(train_dataset_path, \"rb\"))\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True)\n",
    "\n",
    "    test_data = pickle.load(open(test_dataset_path, \"rb\"))\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True)\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "def fine_tune(model, iterations, train_loader, print_frequency=100):\n",
    "    '''\n",
    "        short-term fine-tune a simplified model\n",
    "        \n",
    "        Input:\n",
    "            `model`: model to be fine-tuned.\n",
    "            `iterations`: (int) num of short-term fine-tune iterations.\n",
    "            `print_frequency`: (int) how often to print fine-tune info.\n",
    "        \n",
    "        Output:\n",
    "            `model`: fine-tuned model.\n",
    "    '''\n",
    "\n",
    "    # Data loaders for fine tuning and evaluation.\n",
    "    batch_size = 128\n",
    "    momentum = 0.9\n",
    "    weight_decay = 1e-4\n",
    "    finetune_lr = 0.001\n",
    "\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    _NUM_CLASSES = 10\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        finetune_lr, \n",
    "        momentum=momentum,\n",
    "        weight_decay=weight_decay)\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "    model.train()\n",
    "    dataloader_iter = iter(train_loader)\n",
    "    for i in range(iterations):\n",
    "        try:\n",
    "            (input, target) = next(dataloader_iter)\n",
    "        except:\n",
    "            dataloader_iter = iter(train_loader)\n",
    "            (input, target) = next(dataloader_iter)\n",
    "            \n",
    "        if i % print_frequency == 0:\n",
    "            print('Fine-tuning iteration {}'.format(i))\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        # Ensure the target shape is sth like torch.Size([batch_size])\n",
    "        if len(target.shape) > 1: target = target.reshape(len(target))\n",
    "\n",
    "        target.unsqueeze_(1)\n",
    "        target_onehot = torch.FloatTensor(target.shape[0], _NUM_CLASSES)\n",
    "        target_onehot.zero_()\n",
    "        target_onehot.scatter_(1, target, 1)\n",
    "        target.squeeze_(1)\n",
    "        input, target = input.to(DEVICE), target.to(DEVICE)\n",
    "        target_onehot = target_onehot.to(DEVICE)\n",
    "\n",
    "        pred = model(input)\n",
    "        loss = criterion(pred, target_onehot)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()  # compute gradient and do SGD step\n",
    "        optimizer.step()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"models/alexnet/model_cpu.pth.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Dataset No: 0\n",
      "Fine-tuning iteration 0\n",
      "Fine-tuning iteration 1\n",
      "Fine-tuning iteration 2\n",
      "Fine-tuning iteration 3\n",
      "Fine-tuning iteration 4\n",
      "Fine-tuning iteration 5\n",
      "Fine-tuning iteration 6\n",
      "Fine-tuning iteration 7\n",
      "Fine-tuning iteration 8\n",
      "Fine-tuning iteration 9\n",
      ">> Dataset No: 1\n",
      "Fine-tuning iteration 0\n",
      "Fine-tuning iteration 1\n",
      "Fine-tuning iteration 2\n",
      "Fine-tuning iteration 3\n",
      "Fine-tuning iteration 4\n",
      "Fine-tuning iteration 5\n",
      "Fine-tuning iteration 6\n",
      "Fine-tuning iteration 7\n",
      "Fine-tuning iteration 8\n",
      "Fine-tuning iteration 9\n",
      ">> Dataset No: 2\n",
      "Fine-tuning iteration 0\n",
      "Fine-tuning iteration 1\n",
      "Fine-tuning iteration 2\n",
      "Fine-tuning iteration 3\n",
      "Fine-tuning iteration 4\n",
      "Fine-tuning iteration 5\n",
      "Fine-tuning iteration 6\n",
      "Fine-tuning iteration 7\n",
      "Fine-tuning iteration 8\n",
      "Fine-tuning iteration 9\n",
      ">> Dataset No: 3\n",
      "Fine-tuning iteration 0\n",
      "Fine-tuning iteration 1\n",
      "Fine-tuning iteration 2\n",
      "Fine-tuning iteration 3\n",
      "Fine-tuning iteration 4\n",
      "Fine-tuning iteration 5\n",
      "Fine-tuning iteration 6\n",
      "Fine-tuning iteration 7\n",
      "Fine-tuning iteration 8\n",
      "Fine-tuning iteration 9\n",
      ">> Dataset No: 4\n",
      "Fine-tuning iteration 0\n",
      "Fine-tuning iteration 1\n",
      "Fine-tuning iteration 2\n",
      "Fine-tuning iteration 3\n",
      "Fine-tuning iteration 4\n",
      "Fine-tuning iteration 5\n",
      "Fine-tuning iteration 6\n",
      "Fine-tuning iteration 7\n",
      "Fine-tuning iteration 8\n",
      "Fine-tuning iteration 9\n",
      ">> Dataset No: 5\n",
      "Fine-tuning iteration 0\n",
      "Fine-tuning iteration 1\n",
      "Fine-tuning iteration 2\n",
      "Fine-tuning iteration 3\n",
      "Fine-tuning iteration 4\n",
      "Fine-tuning iteration 5\n",
      "Fine-tuning iteration 6\n",
      "Fine-tuning iteration 7\n",
      "Fine-tuning iteration 8\n",
      "Fine-tuning iteration 9\n",
      ">> Dataset No: 6\n",
      "Fine-tuning iteration 0\n",
      "Fine-tuning iteration 1\n",
      "Fine-tuning iteration 2\n",
      "Fine-tuning iteration 3\n",
      "Fine-tuning iteration 4\n",
      "Fine-tuning iteration 5\n",
      "Fine-tuning iteration 6\n",
      "Fine-tuning iteration 7\n",
      "Fine-tuning iteration 8\n",
      "Fine-tuning iteration 9\n",
      ">> Dataset No: 7\n",
      "Fine-tuning iteration 0\n",
      "Fine-tuning iteration 1\n",
      "Fine-tuning iteration 2\n",
      "Fine-tuning iteration 3\n",
      "Fine-tuning iteration 4\n",
      "Fine-tuning iteration 5\n",
      "Fine-tuning iteration 6\n",
      "Fine-tuning iteration 7\n",
      "Fine-tuning iteration 8\n",
      "Fine-tuning iteration 9\n",
      ">> Dataset No: 8\n",
      "Fine-tuning iteration 0\n",
      "Fine-tuning iteration 1\n",
      "Fine-tuning iteration 2\n",
      "Fine-tuning iteration 3\n",
      "Fine-tuning iteration 4\n",
      "Fine-tuning iteration 5\n",
      "Fine-tuning iteration 6\n",
      "Fine-tuning iteration 7\n",
      "Fine-tuning iteration 8\n",
      "Fine-tuning iteration 9\n",
      ">> Dataset No: 9\n",
      "Fine-tuning iteration 0\n",
      "Fine-tuning iteration 1\n",
      "Fine-tuning iteration 2\n",
      "Fine-tuning iteration 3\n",
      "Fine-tuning iteration 4\n",
      "Fine-tuning iteration 5\n",
      "Fine-tuning iteration 6\n",
      "Fine-tuning iteration 7\n",
      "Fine-tuning iteration 8\n",
      "Fine-tuning iteration 9\n"
     ]
    }
   ],
   "source": [
    "weights_list = []\n",
    "for client_id in range(10):\n",
    "    print(f\">> Dataset No: {client_id}\")\n",
    "    train_dataset_path = f\"./data/Cifar10/train/{client_id}.pkl\"\n",
    "    test_dataset_path = f\"./data/Cifar10/test/{client_id}.pkl\"\n",
    "    trainloader, testloader = load_data(train_dataset_path, test_dataset_path)\n",
    "\n",
    "    temp_model = fine_tune(model, 10, trainloader, print_frequency=1)\n",
    "    weights_model = np.concatenate([param.data.cpu().numpy().flatten() for param in model.parameters()])\n",
    "\n",
    "    weights_list.append(weights_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weights = np.vstack(weights_list)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "cluster_assignments = kmeans.fit_predict(all_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_weights = np.vstack(weights_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/goktug/Desktop/thesis/netadapt-x-flower/nn_cluster.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/nn_cluster.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m kmeans \u001b[39m=\u001b[39m KMeans(n_clusters\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/nn_cluster.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m cluster_assignments \u001b[39m=\u001b[39m kmeans\u001b[39m.\u001b[39;49mfit_predict(all_weights)\n",
      "File \u001b[0;32m~/python_envs/easyfl/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1069\u001b[0m, in \u001b[0;36m_BaseKMeans.fit_predict\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit_predict\u001b[39m(\u001b[39mself\u001b[39m, X, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1047\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute cluster centers and predict cluster index for each sample.\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \n\u001b[1;32m   1049\u001b[0m \u001b[39m    Convenience method; equivalent to calling fit(X) followed by\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1067\u001b[0m \u001b[39m        Index of the cluster each sample belongs to.\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1069\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, sample_weight\u001b[39m=\u001b[39;49msample_weight)\u001b[39m.\u001b[39mlabels_\n",
      "File \u001b[0;32m~/python_envs/easyfl/lib/python3.10/site-packages/sklearn/base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[1;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[1;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[1;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1150\u001b[0m     )\n\u001b[1;32m   1151\u001b[0m ):\n\u001b[0;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/python_envs/easyfl/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1484\u001b[0m, in \u001b[0;36mKMeans.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1449\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Compute k-means clustering.\u001b[39;00m\n\u001b[1;32m   1450\u001b[0m \n\u001b[1;32m   1451\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1473\u001b[0m \u001b[39m    Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1474\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1475\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[1;32m   1476\u001b[0m     X,\n\u001b[1;32m   1477\u001b[0m     accept_sparse\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mcsr\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1481\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1482\u001b[0m )\n\u001b[0;32m-> 1484\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_params_vs_input(X)\n\u001b[1;32m   1486\u001b[0m random_state \u001b[39m=\u001b[39m check_random_state(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state)\n\u001b[1;32m   1487\u001b[0m sample_weight \u001b[39m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[39m=\u001b[39mX\u001b[39m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/python_envs/easyfl/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416\u001b[0m, in \u001b[0;36mKMeans._check_params_vs_input\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1415\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_params_vs_input\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m-> 1416\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_check_params_vs_input(X, default_n_init\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[1;32m   1418\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_algorithm \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malgorithm\n\u001b[1;32m   1419\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_algorithm \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfull\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/python_envs/easyfl/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:871\u001b[0m, in \u001b[0;36m_BaseKMeans._check_params_vs_input\u001b[0;34m(self, X, default_n_init)\u001b[0m\n\u001b[1;32m    866\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    867\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mn_samples=\u001b[39m\u001b[39m{\u001b[39;00mX\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m should be >= n_clusters=\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_clusters\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    868\u001b[0m     )\n\u001b[1;32m    870\u001b[0m \u001b[39m# tol\u001b[39;00m\n\u001b[0;32m--> 871\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tol \u001b[39m=\u001b[39m _tolerance(X, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtol)\n\u001b[1;32m    873\u001b[0m \u001b[39m# n-init\u001b[39;00m\n\u001b[1;32m    874\u001b[0m \u001b[39m# TODO(1.4): Remove\u001b[39;00m\n\u001b[1;32m    875\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_init \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_init\n",
      "File \u001b[0;32m~/python_envs/easyfl/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:290\u001b[0m, in \u001b[0;36m_tolerance\u001b[0;34m(X, tol)\u001b[0m\n\u001b[1;32m    288\u001b[0m     variances \u001b[39m=\u001b[39m mean_variance_axis(X, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m    289\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 290\u001b[0m     variances \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mvar(X, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m    291\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(variances) \u001b[39m*\u001b[39m tol\n",
      "File \u001b[0;32m~/python_envs/easyfl/lib/python3.10/site-packages/numpy/core/fromnumeric.py:3787\u001b[0m, in \u001b[0;36mvar\u001b[0;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[1;32m   3784\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3785\u001b[0m         \u001b[39mreturn\u001b[39;00m var(axis\u001b[39m=\u001b[39maxis, dtype\u001b[39m=\u001b[39mdtype, out\u001b[39m=\u001b[39mout, ddof\u001b[39m=\u001b[39mddof, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 3787\u001b[0m \u001b[39mreturn\u001b[39;00m _methods\u001b[39m.\u001b[39;49m_var(a, axis\u001b[39m=\u001b[39;49maxis, dtype\u001b[39m=\u001b[39;49mdtype, out\u001b[39m=\u001b[39;49mout, ddof\u001b[39m=\u001b[39;49mddof,\n\u001b[1;32m   3788\u001b[0m                      \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/python_envs/easyfl/lib/python3.10/site-packages/numpy/core/_methods.py:187\u001b[0m, in \u001b[0;36m_var\u001b[0;34m(a, axis, dtype, out, ddof, keepdims, where)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[39m# Most general case; includes handling object arrays containing imaginary\u001b[39;00m\n\u001b[1;32m    183\u001b[0m \u001b[39m# numbers and complex types with non-native byteorder\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     x \u001b[39m=\u001b[39m um\u001b[39m.\u001b[39mmultiply(x, um\u001b[39m.\u001b[39mconjugate(x), out\u001b[39m=\u001b[39mx)\u001b[39m.\u001b[39mreal\n\u001b[0;32m--> 187\u001b[0m ret \u001b[39m=\u001b[39m umr_sum(x, axis, dtype, out, keepdims\u001b[39m=\u001b[39;49mkeepdims, where\u001b[39m=\u001b[39;49mwhere)\n\u001b[1;32m    189\u001b[0m \u001b[39m# Compute degrees of freedom and make sure it is not negative.\u001b[39;00m\n\u001b[1;32m    190\u001b[0m rcount \u001b[39m=\u001b[39m um\u001b[39m.\u001b[39mmaximum(rcount \u001b[39m-\u001b[39m ddof, \u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "cluster_assignments = kmeans.fit_predict(all_weights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easyfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
