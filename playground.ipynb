{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/goktug/Desktop/thesis/netadapt-x-flower/playground.ipynb Cell 2\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/playground.ipynb#W1sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/playground.ipynb#W1sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mflowers\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcustomDataset\u001b[39;00m \u001b[39mimport\u001b[39;00m CustomDataset\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/playground.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m data \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(\u001b[39mopen\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39m./data/Cifar10/train/0.pkl\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from flowers.utils.customDataset import CustomDataset\n",
    "\n",
    "data = pickle.load(open(\"./data/Cifar10/train/0.pkl\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataLoader\n",
    "batch_size = 32  # Adjust this as needed\n",
    "dataloader = DataLoader(data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 32, 3)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot handle this data type: (1, 1, 3), <f4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/python_envs/easyfl/lib/python3.10/site-packages/PIL/Image.py:3070\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3069\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3070\u001b[0m     mode, rawmode \u001b[39m=\u001b[39m _fromarray_typemap[typekey]\n\u001b[1;32m   3071\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyError\u001b[0m: ((1, 1, 3), '<f4')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/goktug/Desktop/thesis/netadapt-x-flower/playground.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/playground.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m dataloader\u001b[39m.\u001b[39;49mdataset[\u001b[39m0\u001b[39;49m][\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/Desktop/thesis/netadapt-x-flower/utils/customDataset.py:19\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     17\u001b[0m img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata[idx]\n\u001b[1;32m     18\u001b[0m \u001b[39mprint\u001b[39m(np\u001b[39m.\u001b[39marray(img, dtype \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39muint8)\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> 19\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(np\u001b[39m.\u001b[39marray(img, dtype \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39muint8))\u001b[39m.\u001b[39mconvert(\u001b[39m'\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     22\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(img)\n",
      "File \u001b[0;32m~/python_envs/easyfl/lib/python3.10/site-packages/PIL/Image.py:3073\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3071\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m   3072\u001b[0m         msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mCannot handle this data type: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m typekey\n\u001b[0;32m-> 3073\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m   3074\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3075\u001b[0m     rawmode \u001b[39m=\u001b[39m mode\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot handle this data type: (1, 1, 3), <f4"
     ]
    }
   ],
   "source": [
    "dataloader.dataset[0][\"data\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import ConcatDataset, TensorDataset, DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "from flowers.non_iid_generator.utils.dataset_utils import check, separate_data, split_data, save_file\n",
    "\n",
    "from flowers.utils.customDataset import CustomDataset\n",
    "from argparse import ArgumentParser\n",
    "dir_path = \"./data/Cifar10/\"\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.data[idx]\n",
    "        np_img = np.array(img, dtype = np.uint8)\n",
    "        # print(type(np_img))\n",
    "        # print(np_img.shape)\n",
    "        img = Image.fromarray(np_img, \"RGB\")\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "            # print(\"-->\", type(img))\n",
    "            # print(type(img))\n",
    "            # print(img.shape)\n",
    "\n",
    "        class_id = torch.tensor([self.labels[idx]])\n",
    "\n",
    "        sample = {\n",
    "            'data': img,\n",
    "            'label': self.labels[idx]\n",
    "        }\n",
    "        \n",
    "        return img, class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# transform = transforms.Compose(\n",
    "#     [\n",
    "#         transforms.ToTensor(),\n",
    "#         transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4), \n",
    "    transforms.Resize(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root=dir_path+\"rawdata\", train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root=dir_path+\"rawdata\", train=False, download=True, transform=transform)\n",
    "\n",
    "# ttset = ConcatDataset([trainset, trainset])\n",
    "\n",
    "# ttloader = torch.utils.data.DataLoader(ttset, batch_size=len(trainset.data), shuffle=False)\n",
    "\n",
    "# trainloader = torch.utils.data.DataLoader(\n",
    "#     trainset, batch_size=len(trainset.data), shuffle=False)\n",
    "# testloader = torch.utils.data.DataLoader(\n",
    "#     testset, batch_size=len(testset.data), shuffle=False)\n",
    "\n",
    "dataset_image = []\n",
    "dataset_image.extend(trainset.data)\n",
    "dataset_image.extend(testset.data)\n",
    "dataset_image = np.array(dataset_image)\n",
    "\n",
    "train_targets = trainset.targets\n",
    "test_targets = testset.targets\n",
    "\n",
    "dataset_label = []\n",
    "dataset_label.extend(train_targets)\n",
    "dataset_label.extend(test_targets)\n",
    "dataset_label = np.array(dataset_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "batch_size = 32\n",
    "def save_file(config_path, train_path, test_path, train_data, test_data, num_clients, \n",
    "                num_classes, statistic, niid=False, balance=True, partition=None):\n",
    "    config = {\n",
    "        'num_clients': num_clients, \n",
    "        'num_classes': num_classes, \n",
    "        'non_iid': niid, \n",
    "        'balance': balance, \n",
    "        'partition': partition, \n",
    "        'Size of samples for labels in clients': statistic, \n",
    "        'alpha': alpha, \n",
    "        'batch_size': batch_size, \n",
    "    }\n",
    "\n",
    "    # gc.collect()\n",
    "    print(\"Saving to disk.\\n\")\n",
    "\n",
    "    transform=transforms.Compose([\n",
    "        transforms.RandomCrop(32, padding=4), \n",
    "        transforms.Resize(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "    ])\n",
    "\n",
    "    for idx, train_dict in enumerate(train_data):\n",
    "        data = train_dict[\"x\"]\n",
    "        labels = train_dict[\"y\"]\n",
    "        # tensor_x = torch.Tensor(data) # transform to torch tensor\n",
    "        # tensor_y = torch.Tensor(labels)\n",
    "\n",
    "        # my_dataset = TensorDataset(tensor_x,tensor_y) # create your datset\n",
    "        # my_dataloader = DataLoader(my_dataset) # create your dataloader\n",
    "\n",
    "\n",
    "        # for idx, train_data in enumerate(my_dataloader, 0):\n",
    "        #     data, labels = train_data\n",
    "        #     print(data)\n",
    "\n",
    "\n",
    "        \n",
    "        data = train_dict[\"x\"]\n",
    "        labels = train_dict[\"y\"]\n",
    "        custom_dataset = CustomDataset(data, labels, transform=transform)\n",
    "        print(custom_dataset[0])\n",
    "\n",
    "        with open(train_path + str(idx) + '.pkl', 'wb') as f:\n",
    "            pickle.dump(custom_dataset, f)\n",
    "\n",
    "    # for idx, test_dict in enumerate(test_data):\n",
    "    #     with open(test_path + str(idx) + '.npz', 'wb') as f:\n",
    "    #         np.savez_compressed(f, data=test_dict)\n",
    "    # with open(config_path, 'w') as f:\n",
    "    #     ujson.dump(config, f)\n",
    "\n",
    "    # print(\"Finish generating dataset.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0\t Size of data: 23214\t Labels:  [0 3 4 6 7 8]\n",
      "\t\t Samples of labels:  [(0, 5240), (3, 1303), (4, 5994), (6, 4934), (7, 321), (8, 5422)]\n",
      "--------------------------------------------------\n",
      "Client 1\t Size of data: 16854\t Labels:  [0 1 2 3 4 5 6 7 9]\n",
      "\t\t Samples of labels:  [(0, 31), (1, 258), (2, 5999), (3, 1009), (4, 5), (5, 5999), (6, 1060), (7, 387), (9, 2106)]\n",
      "--------------------------------------------------\n",
      "Client 2\t Size of data: 19932\t Labels:  [0 1 2 3 4 5 6 7 8 9]\n",
      "\t\t Samples of labels:  [(0, 729), (1, 5742), (2, 1), (3, 3688), (4, 1), (5, 1), (6, 6), (7, 5292), (8, 578), (9, 3894)]\n",
      "--------------------------------------------------\n",
      "Total number of samples: 60000\n",
      "The number of train samples: [17410, 12640, 14949]\n",
      "The number of test samples: [5804, 4214, 4983]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_clients = 3\n",
    "num_classes = 10\n",
    "niid = True\n",
    "balance = False\n",
    "partition = \"dir\"\n",
    "\n",
    "X, y, statistic = separate_data((dataset_image, dataset_label), num_clients, num_classes, \n",
    "                                niid, balance, partition)\n",
    "\n",
    "train_data, test_data = split_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving to disk.\n",
      "\n",
      "(tensor([[[-0.8201, -0.8201, -0.8201,  ..., -2.4291, -2.4291, -2.4291],\n",
      "         [-0.8201, -0.8201, -0.8201,  ..., -2.4291, -2.4291, -2.4291],\n",
      "         [-0.8201, -0.8201, -0.8201,  ..., -2.4291, -2.4291, -2.4291],\n",
      "         ...,\n",
      "         [-2.4291, -2.4291, -2.4291,  ..., -2.4291, -2.4291, -2.4291],\n",
      "         [-2.4291, -2.4291, -2.4291,  ..., -2.4291, -2.4291, -2.4291],\n",
      "         [-2.4291, -2.4291, -2.4291,  ..., -2.4291, -2.4291, -2.4291]],\n",
      "\n",
      "        [[-0.3532, -0.3532, -0.3532,  ..., -2.4183, -2.4183, -2.4183],\n",
      "         [-0.3532, -0.3532, -0.3532,  ..., -2.4183, -2.4183, -2.4183],\n",
      "         [-0.3532, -0.3532, -0.3532,  ..., -2.4183, -2.4183, -2.4183],\n",
      "         ...,\n",
      "         [-2.4183, -2.4183, -2.4183,  ..., -2.4183, -2.4183, -2.4183],\n",
      "         [-2.4183, -2.4183, -2.4183,  ..., -2.4183, -2.4183, -2.4183],\n",
      "         [-2.4183, -2.4183, -2.4183,  ..., -2.4183, -2.4183, -2.4183]],\n",
      "\n",
      "        [[-1.1483, -1.1483, -1.1483,  ..., -2.2214, -2.2214, -2.2214],\n",
      "         [-1.1483, -1.1483, -1.1483,  ..., -2.2214, -2.2214, -2.2214],\n",
      "         [-1.1483, -1.1483, -1.1483,  ..., -2.2214, -2.2214, -2.2214],\n",
      "         ...,\n",
      "         [-2.2214, -2.2214, -2.2214,  ..., -2.2214, -2.2214, -2.2214],\n",
      "         [-2.2214, -2.2214, -2.2214,  ..., -2.2214, -2.2214, -2.2214],\n",
      "         [-2.2214, -2.2214, -2.2214,  ..., -2.2214, -2.2214, -2.2214]]]), tensor([7]))\n",
      "(tensor([[[-2.4291, -2.4291, -2.4291,  ..., -2.4291, -2.4291, -2.4291],\n",
      "         [-2.4291, -2.4291, -2.4291,  ..., -2.4291, -2.4291, -2.4291],\n",
      "         [-2.4291, -2.4291, -2.4291,  ..., -2.4291, -2.4291, -2.4291],\n",
      "         ...,\n",
      "         [-2.4291, -2.4291, -2.4291,  ...,  0.4593,  0.4593,  0.4593],\n",
      "         [-2.4291, -2.4291, -2.4291,  ...,  0.4593,  0.4593,  0.4593],\n",
      "         [-2.4291, -2.4291, -2.4291,  ...,  0.4593,  0.4593,  0.4593]],\n",
      "\n",
      "        [[-2.4183, -2.4183, -2.4183,  ..., -2.4183, -2.4183, -2.4183],\n",
      "         [-2.4183, -2.4183, -2.4183,  ..., -2.4183, -2.4183, -2.4183],\n",
      "         [-2.4183, -2.4183, -2.4183,  ..., -2.4183, -2.4183, -2.4183],\n",
      "         ...,\n",
      "         [-2.4183, -2.4183, -2.4183,  ...,  1.0431,  1.0431,  1.0431],\n",
      "         [-2.4183, -2.4183, -2.4183,  ...,  1.0431,  1.0431,  1.0431],\n",
      "         [-2.4183, -2.4183, -2.4183,  ...,  1.0431,  1.0431,  1.0431]],\n",
      "\n",
      "        [[-2.2214, -2.2214, -2.2214,  ..., -2.2214, -2.2214, -2.2214],\n",
      "         [-2.2214, -2.2214, -2.2214,  ..., -2.2214, -2.2214, -2.2214],\n",
      "         [-2.2214, -2.2214, -2.2214,  ..., -2.2214, -2.2214, -2.2214],\n",
      "         ...,\n",
      "         [-2.2214, -2.2214, -2.2214,  ..., -1.3434, -1.3434, -1.3434],\n",
      "         [-2.2214, -2.2214, -2.2214,  ..., -1.3434, -1.3434, -1.3434],\n",
      "         [-2.2214, -2.2214, -2.2214,  ..., -1.3434, -1.3434, -1.3434]]]), tensor([6]))\n",
      "(tensor([[[-2.4291, -2.4291, -2.4291,  ..., -2.4291, -2.4291, -2.4291],\n",
      "         [-2.4291, -2.4291, -2.4291,  ..., -2.4291, -2.4291, -2.4291],\n",
      "         [-2.4291, -2.4291, -2.4291,  ..., -2.4291, -2.4291, -2.4291],\n",
      "         ...,\n",
      "         [-2.4291, -2.4291, -2.4291,  ...,  0.1491,  0.1491,  0.1491],\n",
      "         [-2.4291, -2.4291, -2.4291,  ...,  0.1491,  0.1491,  0.1491],\n",
      "         [-2.4291, -2.4291, -2.4291,  ...,  0.1491,  0.1491,  0.1491]],\n",
      "\n",
      "        [[-2.4183, -2.4183, -2.4183,  ..., -2.4183, -2.4183, -2.4183],\n",
      "         [-2.4183, -2.4183, -2.4183,  ..., -2.4183, -2.4183, -2.4183],\n",
      "         [-2.4183, -2.4183, -2.4183,  ..., -2.4183, -2.4183, -2.4183],\n",
      "         ...,\n",
      "         [-2.4183, -2.4183, -2.4183,  ...,  0.0401,  0.0401,  0.0401],\n",
      "         [-2.4183, -2.4183, -2.4183,  ...,  0.0401,  0.0401,  0.0401],\n",
      "         [-2.4183, -2.4183, -2.4183,  ...,  0.0401,  0.0401,  0.0401]],\n",
      "\n",
      "        [[-2.2214, -2.2214, -2.2214,  ..., -2.2214, -2.2214, -2.2214],\n",
      "         [-2.2214, -2.2214, -2.2214,  ..., -2.2214, -2.2214, -2.2214],\n",
      "         [-2.2214, -2.2214, -2.2214,  ..., -2.2214, -2.2214, -2.2214],\n",
      "         ...,\n",
      "         [-2.2214, -2.2214, -2.2214,  ...,  0.0223,  0.0223,  0.0223],\n",
      "         [-2.2214, -2.2214, -2.2214,  ...,  0.0223,  0.0223,  0.0223],\n",
      "         [-2.2214, -2.2214, -2.2214,  ...,  0.0223,  0.0223,  0.0223]]]), tensor([1]))\n"
     ]
    }
   ],
   "source": [
    "config_path = dir_path + \"config.json\"\n",
    "train_path = dir_path + \"train/\"\n",
    "test_path = dir_path + \"test/\"\n",
    "\n",
    "save_file(config_path, train_path, test_path, train_data, test_data, num_clients, num_classes, \n",
    "        statistic, niid, balance, partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/goktug/Desktop/thesis/netadapt-x-flower/playground.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/playground.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m data \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(\u001b[39mopen\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39m./data/Cifar10/train/0.pkl\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/playground.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m  \u001b[39m# Adjust this as needed\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/playground.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m dataloader \u001b[39m=\u001b[39m DataLoader(data, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "data = pickle.load(open(\"./data/Cifar10/train/0.pkl\", \"rb\"))\n",
    "batch_size = 32  # Adjust this as needed\n",
    "dataloader = DataLoader(data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17410"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "ACC: 0.96875\n",
      "63\n",
      "ACC: 1.96875\n",
      "95\n",
      "ACC: 2.96875\n",
      "126\n",
      "ACC: 3.9375\n",
      "156\n",
      "ACC: 4.875\n",
      "188\n",
      "ACC: 5.875\n",
      "220\n",
      "ACC: 6.875\n",
      "251\n",
      "ACC: 7.84375\n",
      "283\n",
      "ACC: 8.84375\n",
      "315\n",
      "ACC: 9.84375\n",
      "343\n",
      "ACC: 10.71875\n",
      "373\n",
      "ACC: 11.65625\n",
      "404\n",
      "ACC: 12.625\n",
      "436\n",
      "ACC: 13.625\n",
      "467\n",
      "ACC: 14.59375\n",
      "499\n",
      "ACC: 15.59375\n",
      "530\n",
      "ACC: 16.5625\n",
      "561\n",
      "ACC: 17.53125\n",
      "593\n",
      "ACC: 18.53125\n",
      "624\n",
      "ACC: 19.5\n",
      "655\n",
      "ACC: 20.46875\n",
      "686\n",
      "ACC: 21.4375\n",
      "718\n",
      "ACC: 22.4375\n",
      "749\n",
      "ACC: 23.40625\n",
      "781\n",
      "ACC: 24.40625\n",
      "812\n",
      "ACC: 25.375\n",
      "843\n",
      "ACC: 26.34375\n",
      "875\n",
      "ACC: 27.34375\n",
      "906\n",
      "ACC: 28.3125\n",
      "938\n",
      "ACC: 29.3125\n",
      "969\n",
      "ACC: 30.28125\n",
      "1001\n",
      "ACC: 31.28125\n",
      "1032\n",
      "ACC: 32.25\n",
      "1063\n",
      "ACC: 33.21875\n",
      "1095\n",
      "ACC: 34.21875\n",
      "1127\n",
      "ACC: 35.21875\n",
      "1159\n",
      "ACC: 36.21875\n",
      "1190\n",
      "ACC: 37.1875\n",
      "1222\n",
      "ACC: 38.1875\n",
      "1254\n",
      "ACC: 39.1875\n",
      "1286\n",
      "ACC: 40.1875\n",
      "1318\n",
      "ACC: 41.1875\n",
      "1350\n",
      "ACC: 42.1875\n",
      "1382\n",
      "ACC: 43.1875\n",
      "1413\n",
      "ACC: 44.15625\n",
      "1445\n",
      "ACC: 45.15625\n",
      "1474\n",
      "ACC: 46.0625\n",
      "1506\n",
      "ACC: 47.0625\n",
      "1538\n",
      "ACC: 48.0625\n",
      "1570\n",
      "ACC: 49.0625\n",
      "1602\n",
      "ACC: 50.0625\n",
      "1634\n",
      "ACC: 51.0625\n",
      "1664\n",
      "ACC: 52.0\n",
      "1695\n",
      "ACC: 52.96875\n",
      "1727\n",
      "ACC: 53.96875\n",
      "1757\n",
      "ACC: 54.90625\n",
      "1787\n",
      "ACC: 55.84375\n",
      "1819\n",
      "ACC: 56.84375\n",
      "1850\n",
      "ACC: 57.8125\n",
      "1882\n",
      "ACC: 58.8125\n",
      "1913\n",
      "ACC: 59.78125\n",
      "1945\n",
      "ACC: 60.78125\n",
      "1977\n",
      "ACC: 61.78125\n",
      "2008\n",
      "ACC: 62.75\n",
      "2040\n",
      "ACC: 63.75\n",
      "2071\n",
      "ACC: 64.71875\n",
      "2102\n",
      "ACC: 65.6875\n",
      "2134\n",
      "ACC: 66.6875\n",
      "2166\n",
      "ACC: 67.6875\n",
      "2196\n",
      "ACC: 68.625\n",
      "2226\n",
      "ACC: 69.5625\n",
      "2258\n",
      "ACC: 70.5625\n",
      "2289\n",
      "ACC: 71.53125\n",
      "2319\n",
      "ACC: 72.46875\n",
      "2351\n",
      "ACC: 73.46875\n",
      "2383\n",
      "ACC: 74.46875\n",
      "2415\n",
      "ACC: 75.46875\n",
      "2445\n",
      "ACC: 76.40625\n",
      "2477\n",
      "ACC: 77.40625\n",
      "2508\n",
      "ACC: 78.375\n",
      "2537\n",
      "ACC: 79.28125\n",
      "2569\n",
      "ACC: 80.28125\n",
      "2601\n",
      "ACC: 81.28125\n",
      "2633\n",
      "ACC: 82.28125\n",
      "2665\n",
      "ACC: 83.28125\n",
      "2697\n",
      "ACC: 84.28125\n",
      "2729\n",
      "ACC: 85.28125\n",
      "2761\n",
      "ACC: 86.28125\n",
      "2793\n",
      "ACC: 87.28125\n",
      "2824\n",
      "ACC: 88.25\n",
      "2856\n",
      "ACC: 89.25\n",
      "2888\n",
      "ACC: 90.25\n",
      "2920\n",
      "ACC: 91.25\n",
      "2949\n",
      "ACC: 92.15625\n",
      "2981\n",
      "ACC: 93.15625\n",
      "3013\n",
      "ACC: 94.15625\n",
      "3044\n",
      "ACC: 95.125\n",
      "3076\n",
      "ACC: 96.125\n",
      "3105\n",
      "ACC: 97.03125\n",
      "3137\n",
      "ACC: 98.03125\n",
      "3168\n",
      "ACC: 99.0\n",
      "3199\n",
      "ACC: 99.96875\n",
      "3230\n",
      "ACC: 100.9375\n",
      "3261\n",
      "ACC: 101.90625\n",
      "3293\n",
      "ACC: 102.90625\n",
      "3321\n",
      "ACC: 103.78125\n",
      "3353\n",
      "ACC: 104.78125\n",
      "3385\n",
      "ACC: 105.78125\n",
      "3415\n",
      "ACC: 106.71875\n",
      "3445\n",
      "ACC: 107.65625\n",
      "3477\n",
      "ACC: 108.65625\n",
      "3509\n",
      "ACC: 109.65625\n",
      "3541\n",
      "ACC: 110.65625\n",
      "3573\n",
      "ACC: 111.65625\n",
      "3605\n",
      "ACC: 112.65625\n",
      "3636\n",
      "ACC: 113.625\n",
      "3667\n",
      "ACC: 114.59375\n",
      "3698\n",
      "ACC: 115.5625\n",
      "3730\n",
      "ACC: 116.5625\n",
      "3761\n",
      "ACC: 117.53125\n",
      "3791\n",
      "ACC: 118.46875\n",
      "3823\n",
      "ACC: 119.46875\n",
      "3855\n",
      "ACC: 120.46875\n",
      "3884\n",
      "ACC: 121.375\n",
      "3914\n",
      "ACC: 122.3125\n",
      "3946\n",
      "ACC: 123.3125\n",
      "3978\n",
      "ACC: 124.3125\n",
      "4010\n",
      "ACC: 125.3125\n",
      "4042\n",
      "ACC: 126.3125\n",
      "4073\n",
      "ACC: 127.28125\n",
      "4105\n",
      "ACC: 128.28125\n",
      "4136\n",
      "ACC: 129.25\n",
      "4168\n",
      "ACC: 130.25\n",
      "4199\n",
      "ACC: 131.21875\n",
      "4231\n",
      "ACC: 132.21875\n",
      "4263\n",
      "ACC: 133.21875\n",
      "4295\n",
      "ACC: 134.21875\n",
      "4326\n",
      "ACC: 135.1875\n",
      "4358\n",
      "ACC: 136.1875\n",
      "4390\n",
      "ACC: 137.1875\n",
      "4421\n",
      "ACC: 138.15625\n",
      "4452\n",
      "ACC: 139.125\n",
      "4483\n",
      "ACC: 140.09375\n",
      "4515\n",
      "ACC: 141.09375\n",
      "4547\n",
      "ACC: 142.09375\n",
      "4579\n",
      "ACC: 143.09375\n",
      "4610\n",
      "ACC: 144.0625\n",
      "4642\n",
      "ACC: 145.0625\n",
      "4674\n",
      "ACC: 146.0625\n",
      "4705\n",
      "ACC: 147.03125\n",
      "4737\n",
      "ACC: 148.03125\n",
      "4768\n",
      "ACC: 149.0\n",
      "4799\n",
      "ACC: 149.96875\n",
      "4829\n",
      "ACC: 150.90625\n",
      "4859\n",
      "ACC: 151.84375\n",
      "4889\n",
      "ACC: 152.78125\n",
      "4921\n",
      "ACC: 153.78125\n",
      "4952\n",
      "ACC: 154.75\n",
      "4984\n",
      "ACC: 155.75\n",
      "5016\n",
      "ACC: 156.75\n",
      "5047\n",
      "ACC: 157.71875\n",
      "5077\n",
      "ACC: 158.65625\n",
      "5108\n",
      "ACC: 159.625\n",
      "5140\n",
      "ACC: 160.625\n",
      "5171\n",
      "ACC: 161.59375\n",
      "5203\n",
      "ACC: 162.59375\n",
      "5235\n",
      "ACC: 163.59375\n",
      "5266\n",
      "ACC: 164.5625\n",
      "5298\n",
      "ACC: 165.5625\n",
      "5328\n",
      "ACC: 166.5\n",
      "5359\n",
      "ACC: 167.46875\n",
      "5391\n",
      "ACC: 168.46875\n",
      "5422\n",
      "ACC: 169.4375\n",
      "5454\n",
      "ACC: 170.4375\n",
      "5485\n",
      "ACC: 171.40625\n",
      "5517\n",
      "ACC: 172.40625\n",
      "5549\n",
      "ACC: 173.40625\n",
      "5580\n",
      "ACC: 174.375\n",
      "5610\n",
      "ACC: 175.3125\n",
      "5642\n",
      "ACC: 176.3125\n",
      "5674\n",
      "ACC: 177.3125\n",
      "5706\n",
      "ACC: 178.3125\n",
      "5737\n",
      "ACC: 179.28125\n",
      "5769\n",
      "ACC: 180.28125\n",
      "5801\n",
      "ACC: 181.28125\n",
      "5833\n",
      "ACC: 182.28125\n",
      "5864\n",
      "ACC: 183.25\n",
      "5896\n",
      "ACC: 184.25\n",
      "5928\n",
      "ACC: 185.25\n",
      "5959\n",
      "ACC: 186.21875\n",
      "5991\n",
      "ACC: 187.21875\n",
      "6022\n",
      "ACC: 188.1875\n",
      "6053\n",
      "ACC: 189.15625\n",
      "6084\n",
      "ACC: 190.125\n",
      "6114\n",
      "ACC: 191.0625\n",
      "6146\n",
      "ACC: 192.0625\n",
      "6177\n",
      "ACC: 193.03125\n",
      "6209\n",
      "ACC: 194.03125\n",
      "6238\n",
      "ACC: 194.9375\n",
      "6270\n",
      "ACC: 195.9375\n",
      "6300\n",
      "ACC: 196.875\n",
      "6330\n",
      "ACC: 197.8125\n",
      "6361\n",
      "ACC: 198.78125\n",
      "6393\n",
      "ACC: 199.78125\n",
      "6424\n",
      "ACC: 200.75\n",
      "6455\n",
      "ACC: 201.71875\n",
      "6487\n",
      "ACC: 202.71875\n",
      "6517\n",
      "ACC: 203.65625\n",
      "6548\n",
      "ACC: 204.625\n",
      "6580\n",
      "ACC: 205.625\n",
      "6612\n",
      "ACC: 206.625\n",
      "6644\n",
      "ACC: 207.625\n",
      "6676\n",
      "ACC: 208.625\n",
      "6707\n",
      "ACC: 209.59375\n",
      "6739\n",
      "ACC: 210.59375\n",
      "6770\n",
      "ACC: 211.5625\n",
      "6801\n",
      "ACC: 212.53125\n",
      "6831\n",
      "ACC: 213.46875\n",
      "6863\n",
      "ACC: 214.46875\n",
      "6895\n",
      "ACC: 215.46875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/goktug/Desktop/thesis/netadapt-x-flower/playground.ipynb Cell 13\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/playground.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m correct, loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m, \u001b[39m0.0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/playground.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/playground.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mfor\u001b[39;00m images, labels \u001b[39min\u001b[39;00m dataloader:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/playground.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/playground.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39m# labels.unsqueeze_(1)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/playground.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         target_onehot \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor(labels\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m10\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/playground.ipynb#X14sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m         target_onehot\u001b[39m.\u001b[39mzero_()\n",
      "File \u001b[0;32m~/python_envs/easyfl/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/python_envs/easyfl/lib/python3.10/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/python_envs/easyfl/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/python_envs/easyfl/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/home/goktug/Desktop/thesis/netadapt-x-flower/playground.ipynb Cell 13\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/playground.ipynb#X14sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(np_img, \u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/playground.ipynb#X14sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/playground.ipynb#X14sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/playground.ipynb#X14sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39m# print(\"-->\", type(img))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/playground.ipynb#X14sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39m# print(type(img))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/playground.ipynb#X14sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     \u001b[39m# print(img.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/goktug/Desktop/thesis/netadapt-x-flower/playground.ipynb#X14sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m class_id \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels[idx]])\n",
      "File \u001b[0;32m~/python_envs/easyfl/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/python_envs/easyfl/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/python_envs/easyfl/lib/python3.10/site-packages/torchvision/transforms/transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m    354\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mresize(img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mantialias)\n",
      "File \u001b[0;32m~/python_envs/easyfl/lib/python3.10/site-packages/torchvision/transforms/functional.py:490\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    488\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    489\u001b[0m     pil_interpolation \u001b[39m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m--> 490\u001b[0m     \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39;49mresize(img, size\u001b[39m=\u001b[39;49moutput_size, interpolation\u001b[39m=\u001b[39;49mpil_interpolation)\n\u001b[1;32m    492\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39mresize(img, size\u001b[39m=\u001b[39moutput_size, interpolation\u001b[39m=\u001b[39minterpolation\u001b[39m.\u001b[39mvalue, antialias\u001b[39m=\u001b[39mantialias)\n",
      "File \u001b[0;32m~/python_envs/easyfl/lib/python3.10/site-packages/torchvision/transforms/_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(size, \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(size) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot inappropriate size arg: \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 250\u001b[0m \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mresize(\u001b[39mtuple\u001b[39;49m(size[::\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]), interpolation)\n",
      "File \u001b[0;32m~/python_envs/easyfl/lib/python3.10/site-packages/PIL/Image.py:2174\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2166\u001b[0m             \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mreduce(\u001b[39mself\u001b[39m, factor, box\u001b[39m=\u001b[39mreduce_box)\n\u001b[1;32m   2167\u001b[0m         box \u001b[39m=\u001b[39m (\n\u001b[1;32m   2168\u001b[0m             (box[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m0\u001b[39m]) \u001b[39m/\u001b[39m factor_x,\n\u001b[1;32m   2169\u001b[0m             (box[\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m1\u001b[39m]) \u001b[39m/\u001b[39m factor_y,\n\u001b[1;32m   2170\u001b[0m             (box[\u001b[39m2\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m0\u001b[39m]) \u001b[39m/\u001b[39m factor_x,\n\u001b[1;32m   2171\u001b[0m             (box[\u001b[39m3\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m1\u001b[39m]) \u001b[39m/\u001b[39m factor_y,\n\u001b[1;32m   2172\u001b[0m         )\n\u001b[0;32m-> 2174\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mim\u001b[39m.\u001b[39;49mresize(size, resample, box))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = torch.load(\"models/alexnet/model_pt21.pth.tar\")\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "correct, loss = 0, 0.0\n",
    "with torch.no_grad():\n",
    "    for images, labels in dataloader:\n",
    "        \n",
    "        # labels.unsqueeze_(1)\n",
    "        target_onehot = torch.FloatTensor(labels.shape[0], 10)\n",
    "        target_onehot.zero_()\n",
    "        target_onehot.scatter_(1, labels, 1)\n",
    "        # labels.squeeze_(1)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        outputs = net(images.to(DEVICE))\n",
    "        labels_one_hot = target_onehot.to(DEVICE)\n",
    "\n",
    "        loss += criterion(outputs, labels_one_hot).item()\n",
    "        correct += (torch.max(outputs.data, 1)[1] == labels.squeeze_(1)).sum().item()\n",
    "        # print(torch.max(outputs.data, 1)[1].shape)\n",
    "        # print(labels.squeeze(1).shape)\n",
    "        # print((torch.max(outputs.data, 1)[1] == labels.squeeze(1)).shape)\n",
    "        print(correct)\n",
    "        print(f\"ACC: {correct / len(dataloader.dataset)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easyfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
