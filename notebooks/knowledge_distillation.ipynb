{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    " \n",
    "# setting path\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "from non_iid_generator.customDataset import CustomDataset\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Check if GPU is available, and if not, use the CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_knowledge_distillation(teacher, student, train_loader, epochs, learning_rate, T, soft_target_loss_weight, ce_loss_weight, device):\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(student.parameters(), lr=learning_rate)\n",
    "\n",
    "    teacher.eval()  # Teacher set to evaluation mode\n",
    "    student.train() # Student to train mode\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # labels = torch.nn.functional.one_hot(labels, 10).to(device)\n",
    "            labels = labels.squeeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass with the teacher model - do not save gradients here as we do not change the teacher's weights\n",
    "            with torch.no_grad():\n",
    "                teacher_logits = teacher(inputs)\n",
    "\n",
    "            # Forward pass with the student model\n",
    "            student_logits = student(inputs)\n",
    "\n",
    "\n",
    "\n",
    "            #Soften the student logits by applying softmax first and log() second\n",
    "            soft_targets = nn.functional.softmax(teacher_logits / T, dim=-1)\n",
    "            soft_prob = nn.functional.log_softmax(student_logits / T, dim=-1)\n",
    "\n",
    "            # Calculate the soft targets loss. Scaled by T**2 as suggested by the authors of the paper \"Distilling the knowledge in a neural network\"\n",
    "            soft_targets_loss = -torch.sum(soft_targets * soft_prob) / soft_prob.size()[0] * (T**2)\n",
    "\n",
    "            # Calculate the true label loss\n",
    "            # print(labels)\n",
    "            # print(len(labels))\n",
    "            label_loss = ce_loss(student_logits, labels)\n",
    "\n",
    "            # Weighted sum of the two losses\n",
    "            loss = soft_target_loss_weight * soft_targets_loss + ce_loss_weight * label_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "def train(model, train_loader, epochs, learning_rate, device):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            # inputs: A collection of batch_size images\n",
    "            # labels: A vector of dimensionality batch_size with integers denoting class of each image\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.squeeze(1).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # outputs: Output of the network for the collection of images. A tensor of dimensionality batch_size x num_classes\n",
    "            # labels: The actual labels of the images. Vector of dimensionality batch_size\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.squeeze(1).to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted.data == labels).sum().item()\n",
    "            # print(predicted.data)\n",
    "            # correct += (torch.max(predicted.data, 1)[1] == labels.squeeze_(1)).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_dataset_path, test_dataset_path):\n",
    "    \"\"\"Load CIFAR-10 (training and test set).\"\"\"\n",
    "    # trf = Compose([ToTensor(), Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    # trainset = CIFAR10(\"./data\", train=True, download=True, transform=trf)\n",
    "    # testset = CIFAR10(\"./data\", train=False, download=True, transform=trf)\n",
    "    # return DataLoader(trainset, batch_size=32, shuffle=True), DataLoader(testset)\n",
    "\n",
    "    batch_size = 128\n",
    "    momentum = 0.9\n",
    "    weight_decay = 1e-4\n",
    "    finetune_lr = 0.001\n",
    "\n",
    "    train_data = pickle.load(open(train_dataset_path, \"rb\"))\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True)\n",
    "\n",
    "    test_data = pickle.load(open(test_dataset_path, \"rb\"))\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 53.85%\n",
      "Epoch 1/50, Loss: 22.153010654449464\n",
      "Epoch 2/50, Loss: 1.1728242337703705\n",
      "Epoch 3/50, Loss: 1.1214742064476013\n",
      "Epoch 4/50, Loss: 1.0863099217414856\n",
      "Epoch 5/50, Loss: 1.0780147552490233\n",
      "Epoch 6/50, Loss: 1.0762548267841339\n",
      "Epoch 7/50, Loss: 1.0806386351585389\n",
      "Epoch 8/50, Loss: 1.088597047328949\n",
      "Epoch 9/50, Loss: 1.067915803194046\n",
      "Epoch 10/50, Loss: 1.049210751056671\n",
      "Epoch 11/50, Loss: 1.0614818394184113\n",
      "Epoch 12/50, Loss: 1.0126095354557036\n",
      "Epoch 13/50, Loss: 0.9615123510360718\n",
      "Epoch 14/50, Loss: 0.9356087446212769\n",
      "Epoch 15/50, Loss: 0.853547728061676\n",
      "Epoch 16/50, Loss: 0.807304835319519\n",
      "Epoch 17/50, Loss: 0.8125032424926758\n",
      "Epoch 18/50, Loss: 0.7153206050395966\n",
      "Epoch 19/50, Loss: 0.7078683733940124\n",
      "Epoch 20/50, Loss: 0.6943287342786789\n",
      "Epoch 21/50, Loss: 0.684327346086502\n",
      "Epoch 22/50, Loss: 0.6577076375484466\n",
      "Epoch 23/50, Loss: 0.6471943259239197\n",
      "Epoch 24/50, Loss: 0.6479382574558258\n",
      "Epoch 25/50, Loss: 0.6264076679944992\n",
      "Epoch 26/50, Loss: 0.6335215121507645\n",
      "Epoch 27/50, Loss: 0.5703027129173279\n",
      "Epoch 28/50, Loss: 0.6000054270029068\n",
      "Epoch 29/50, Loss: 0.5458011060953141\n",
      "Epoch 30/50, Loss: 0.5444089412689209\n",
      "Epoch 31/50, Loss: 0.49061035811901094\n",
      "Epoch 32/50, Loss: 0.47732971906661986\n",
      "Epoch 33/50, Loss: 0.4553307890892029\n",
      "Epoch 34/50, Loss: 0.46624980866909027\n",
      "Epoch 35/50, Loss: 0.48108177781105044\n",
      "Epoch 36/50, Loss: 0.4284108251333237\n",
      "Epoch 37/50, Loss: 0.4109829843044281\n",
      "Epoch 38/50, Loss: 0.38345048725605013\n",
      "Epoch 39/50, Loss: 0.35877419710159303\n",
      "Epoch 40/50, Loss: 0.38548182547092436\n",
      "Epoch 41/50, Loss: 0.38481259942054746\n",
      "Epoch 42/50, Loss: 0.36449541449546813\n",
      "Epoch 43/50, Loss: 0.35493487268686297\n",
      "Epoch 44/50, Loss: 0.3134810343384743\n",
      "Epoch 45/50, Loss: 0.294462089240551\n",
      "Epoch 46/50, Loss: 0.26669957786798476\n",
      "Epoch 47/50, Loss: 0.24460585564374923\n",
      "Epoch 48/50, Loss: 0.2256958857178688\n",
      "Epoch 49/50, Loss: 0.2092016376554966\n",
      "Epoch 50/50, Loss: 0.2221104010939598\n",
      "Test Accuracy: 73.95%\n",
      "Teacher accuracy: 53.85%\n",
      "Student accuracy: 73.95%\n",
      "Epoch 1/50, Loss: 54.713116836547854\n",
      "Epoch 2/50, Loss: 3.3706456899642943\n",
      "Epoch 3/50, Loss: 3.186039447784424\n",
      "Epoch 4/50, Loss: 3.1319945812225343\n",
      "Epoch 5/50, Loss: 3.139960789680481\n",
      "Epoch 6/50, Loss: 3.121447801589966\n",
      "Epoch 7/50, Loss: 3.1277950525283815\n",
      "Epoch 8/50, Loss: 3.1195804119110107\n",
      "Epoch 9/50, Loss: 3.0965481996536255\n",
      "Epoch 10/50, Loss: 3.0968747615814207\n",
      "Epoch 11/50, Loss: 3.0898337841033934\n",
      "Epoch 12/50, Loss: 3.056523871421814\n",
      "Epoch 13/50, Loss: 3.0162901878356934\n",
      "Epoch 14/50, Loss: 3.0608914852142335\n",
      "Epoch 15/50, Loss: 2.949227142333984\n",
      "Epoch 16/50, Loss: 2.8750770807266237\n",
      "Epoch 17/50, Loss: 2.7642531871795653\n",
      "Epoch 18/50, Loss: 2.666843605041504\n",
      "Epoch 19/50, Loss: 2.6392163276672362\n",
      "Epoch 20/50, Loss: 2.6329514741897584\n",
      "Epoch 21/50, Loss: 2.5661651134490966\n",
      "Epoch 22/50, Loss: 2.5340100288391114\n",
      "Epoch 23/50, Loss: 2.546890449523926\n",
      "Epoch 24/50, Loss: 2.516058158874512\n",
      "Epoch 25/50, Loss: 2.4803404092788695\n",
      "Epoch 26/50, Loss: 2.4309149503707888\n",
      "Epoch 27/50, Loss: 2.4433542013168337\n",
      "Epoch 28/50, Loss: 2.4066076040267945\n",
      "Epoch 29/50, Loss: 2.4022268295288085\n",
      "Epoch 30/50, Loss: 2.387420320510864\n",
      "Epoch 31/50, Loss: 2.375688910484314\n",
      "Epoch 32/50, Loss: 2.3808464288711546\n",
      "Epoch 33/50, Loss: 2.3330785512924193\n",
      "Epoch 34/50, Loss: 2.3247454166412354\n",
      "Epoch 35/50, Loss: 2.327222394943237\n",
      "Epoch 36/50, Loss: 2.2911674976348877\n",
      "Epoch 37/50, Loss: 2.3232022285461427\n",
      "Epoch 38/50, Loss: 2.2824865579605103\n",
      "Epoch 39/50, Loss: 2.3268378257751463\n",
      "Epoch 40/50, Loss: 2.2955931425094604\n",
      "Epoch 41/50, Loss: 2.2883138418197633\n",
      "Epoch 42/50, Loss: 2.328203248977661\n",
      "Epoch 43/50, Loss: 2.3711841106414795\n",
      "Epoch 44/50, Loss: 2.2621337890625\n",
      "Epoch 45/50, Loss: 2.255536603927612\n",
      "Epoch 46/50, Loss: 2.22961106300354\n",
      "Epoch 47/50, Loss: 2.2287970542907716\n",
      "Epoch 48/50, Loss: 2.2251827716827393\n",
      "Epoch 49/50, Loss: 2.20898060798645\n",
      "Epoch 50/50, Loss: 2.1993271350860595\n",
      "Test Accuracy: 77.42%\n",
      "Teacher accuracy: 53.85%\n",
      "Student accuracy without teacher: 73.95%\n",
      "Student accuracy with CE + KD: 77.42%\n"
     ]
    }
   ],
   "source": [
    "client_id = 3\n",
    "train_dataset_path = f\"../data/32_Cifar10_NIID_56c_a03/train/{client_id}.pkl\"\n",
    "test_dataset_path = f\"../data/32_Cifar10_NIID_56c_a03/test/{client_id}.pkl\"\n",
    "train_loader, test_loader = load_data(train_dataset_path, test_dataset_path)\n",
    "\n",
    "nn_deep = torch.load(\"../projects/define_pretrained_fed_sim_NIID_alpha03/last_model.pth.tar\")\n",
    "nn_light = torch.load(\"../projects/test/test-4/master/iter_5_best_model.pth.tar\")\n",
    "\n",
    "test_accuracy_deep = test(nn_deep, test_loader, device)\n",
    "\n",
    "train(nn_light, train_loader, epochs=50, learning_rate=0.001, device=device)\n",
    "test_accuracy_light_ce = test(nn_light, test_loader, device)\n",
    "\n",
    "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
    "print(f\"Student accuracy: {test_accuracy_light_ce:.2f}%\")\n",
    "\n",
    "###########################\n",
    "###########################\n",
    "\n",
    "\n",
    "new_nn_light = torch.load(\"../projects/test/test-4/master/iter_5_best_model.pth.tar\")\n",
    "# Apply ``train_knowledge_distillation`` with a temperature of 2. Arbitrarily set the weights to 0.75 for CE and 0.25 for distillation loss.\n",
    "train_knowledge_distillation(teacher=nn_deep, student=new_nn_light, train_loader=train_loader, epochs=50, learning_rate=0.001, T=2, soft_target_loss_weight=0.25, ce_loss_weight=0.75, device=device)\n",
    "test_accuracy_light_ce_and_kd = test(new_nn_light, test_loader, device)\n",
    "\n",
    "# Compare the student test accuracy with and without the teacher, after distillation\n",
    "print(f\"Teacher accuracy: {test_accuracy_deep:.2f}%\")\n",
    "print(f\"Student accuracy without teacher: {test_accuracy_light_ce:.2f}%\")\n",
    "print(f\"Student accuracy with CE + KD: {test_accuracy_light_ce_and_kd:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easyfl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
